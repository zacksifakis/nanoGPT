
step 1500: train loss 1.1477, val loss 1.4674
iter 1500: loss 1.2321, time 22919.52ms, mfu -100.00%
iter 1510: loss 1.2169, time 46.40ms, mfu 8.03%
iter 1520: loss 1.2432, time 46.42ms, mfu 8.03%
iter 1530: loss 1.2366, time 46.85ms, mfu 8.02%
iter 1540: loss 1.2264, time 46.25ms, mfu 8.03%
iter 1550: loss 1.2111, time 46.31ms, mfu 8.03%
iter 1560: loss 1.2161, time 46.31ms, mfu 8.03%
iter 1570: loss 1.2097, time 46.40ms, mfu 8.03%
iter 1580: loss 1.2148, time 46.39ms, mfu 8.03%
iter 1590: loss 1.2271, time 46.48ms, mfu 8.03%
iter 1600: loss 1.2189, time 46.14ms, mfu 8.03%
iter 1610: loss 1.2018, time 46.43ms, mfu 8.03%
iter 1620: loss 1.1904, time 47.00ms, mfu 8.02%
iter 1630: loss 1.1953, time 47.40ms, mfu 8.01%
iter 1640: loss 1.2136, time 46.65ms, mfu 8.00%
iter 1650: loss 1.2035, time 46.78ms, mfu 8.00%
iter 1660: loss 1.2234, time 47.05ms, mfu 7.99%
iter 1670: loss 1.1913, time 46.46ms, mfu 8.00%
iter 1680: loss 1.2095, time 46.64ms, mfu 7.99%
iter 1690: loss 1.2041, time 46.68ms, mfu 7.99%
iter 1700: loss 1.2021, time 46.83ms, mfu 7.99%
iter 1710: loss 1.1993, time 47.48ms, mfu 7.98%
iter 1720: loss 1.2068, time 47.08ms, mfu 7.97%
iter 1730: loss 1.1735, time 46.78ms, mfu 7.97%
iter 1740: loss 1.1726, time 46.70ms, mfu 7.97%
step 1750: train loss 1.1037, val loss 1.4743
iter 1750: loss 1.1492, time 18896.68ms, mfu 7.18%
iter 1760: loss 1.1689, time 46.64ms, mfu 7.26%
iter 1770: loss 1.1764, time 46.05ms, mfu 7.34%
iter 1780: loss 1.1834, time 46.44ms, mfu 7.41%
iter 1790: loss 1.2139, time 46.18ms, mfu 7.47%
iter 1800: loss 1.1972, time 46.61ms, mfu 7.53%
iter 1810: loss 1.2100, time 47.13ms, mfu 7.56%
iter 1820: loss 1.1607, time 47.37ms, mfu 7.59%
iter 1830: loss 1.1742, time 46.55ms, mfu 7.64%
iter 1840: loss 1.1631, time 46.35ms, mfu 7.68%
iter 1850: loss 1.1427, time 46.19ms, mfu 7.72%
iter 1860: loss 1.1416, time 46.56ms, mfu 7.74%
iter 1870: loss 1.1753, time 46.56ms, mfu 7.77%
iter 1880: loss 1.1738, time 46.74ms, mfu 7.79%
iter 1890: loss 1.1477, time 46.77ms, mfu 7.81%
iter 1900: loss 1.1562, time 46.73ms, mfu 7.82%
iter 1910: loss 1.1139, time 46.72ms, mfu 7.84%
iter 1920: loss 1.1692, time 46.54ms, mfu 7.86%
iter 1930: loss 1.1853, time 46.50ms, mfu 7.87%
iter 1940: loss 1.1394, time 46.66ms, mfu 7.88%
iter 1950: loss 1.1562, time 46.92ms, mfu 7.89%
iter 1960: loss 1.1312, time 47.17ms, mfu 7.89%
iter 1970: loss 1.1490, time 46.68ms, mfu 7.90%
iter 1980: loss 1.1433, time 46.76ms, mfu 7.91%
iter 1990: loss 1.1252, time 46.89ms, mfu 7.91%
step 2000: train loss 1.0550, val loss 1.4723
iter 2000: loss 1.1662, time 19298.29ms, mfu 7.12%
iter 2010: loss 1.1325, time 46.21ms, mfu 7.22%
iter 2020: loss 1.1392, time 47.23ms, mfu 7.28%
iter 2030: loss 1.1278, time 46.56ms, mfu 7.36%
iter 2040: loss 1.1444, time 46.53ms, mfu 7.42%
iter 2050: loss 1.1285, time 46.64ms, mfu 7.48%
iter 2060: loss 1.1549, time 46.82ms, mfu 7.53%
iter 2070: loss 1.1105, time 46.87ms, mfu 7.57%
iter 2080: loss 1.1091, time 47.53ms, mfu 7.60%
iter 2090: loss 1.1447, time 46.85ms, mfu 7.63%
iter 2100: loss 1.1360, time 46.57ms, mfu 7.67%
iter 2110: loss 1.1191, time 46.80ms, mfu 7.70%
iter 2120: loss 1.1077, time 46.60ms, mfu 7.73%
iter 2130: loss 1.1189, time 47.07ms, mfu 7.75%
iter 2140: loss 1.0791, time 47.46ms, mfu 7.76%
iter 2150: loss 1.1326, time 47.13ms, mfu 7.77%
iter 2160: loss 1.0892, time 46.86ms, mfu 7.79%
iter 2170: loss 1.1499, time 46.56ms, mfu 7.81%
iter 2180: loss 1.1120, time 46.73ms, mfu 7.83%
iter 2190: loss 1.1000, time 47.21ms, mfu 7.83%
iter 2200: loss 1.1073, time 47.14ms, mfu 7.84%
iter 2210: loss 1.0484, time 46.66ms, mfu 7.86%
iter 2220: loss 1.0727, time 46.99ms, mfu 7.86%
iter 2230: loss 1.1048, time 47.25ms, mfu 7.87%
iter 2240: loss 1.1005, time 46.88ms, mfu 7.87%
step 2250: train loss 1.0032, val loss 1.4825
iter 2250: loss 1.0889, time 18966.94ms, mfu 7.09%
iter 2260: loss 1.1112, time 46.80ms, mfu 7.18%
iter 2270: loss 1.0757, time 46.94ms, mfu 7.25%
iter 2280: loss 1.0793, time 46.63ms, mfu 7.33%
iter 2290: loss 1.0878, time 47.35ms, mfu 7.38%
iter 2300: loss 1.0834, time 46.54ms, mfu 7.44%
iter 2310: loss 1.0805, time 46.71ms, mfu 7.50%
iter 2320: loss 1.0765, time 46.21ms, mfu 7.55%
iter 2330: loss 1.0713, time 46.38ms, mfu 7.60%
iter 2340: loss 1.0827, time 47.09ms, mfu 7.63%
iter 2350: loss 1.0617, time 47.10ms, mfu 7.66%
iter 2360: loss 1.0754, time 46.60ms, mfu 7.69%
iter 2370: loss 1.0876, time 46.66ms, mfu 7.72%
iter 2380: loss 1.0897, time 46.83ms, mfu 7.75%
iter 2390: loss 1.0461, time 46.46ms, mfu 7.77%
iter 2400: loss 1.0567, time 47.60ms, mfu 7.78%
iter 2410: loss 1.0920, time 47.11ms, mfu 7.79%
iter 2420: loss 1.0751, time 46.93ms, mfu 7.81%
iter 2430: loss 1.0798, time 46.77ms, mfu 7.82%
iter 2440: loss 1.0937, time 47.51ms, mfu 7.83%
iter 2450: loss 1.0649, time 47.05ms, mfu 7.83%
iter 2460: loss 1.0755, time 47.24ms, mfu 7.84%
iter 2470: loss 1.0278, time 46.85ms, mfu 7.85%
iter 2480: loss 1.0609, time 47.15ms, mfu 7.86%
iter 2490: loss 1.0963, time 46.91ms, mfu 7.87%
[2023-10-13 01:16:00,022] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (8)
[2023-10-13 01:16:00,022] torch._dynamo.convert_frame: [WARNING]    function: 'forward' (/home/workspace/model.py:170)
[2023-10-13 01:16:00,022] torch._dynamo.convert_frame: [WARNING] to diagnose recompilation issues, set env variable TORCHDYNAMO_REPORT_GUARD_FAILURES=1 and also see https://pytorch.org/docs/master/compile/troubleshooting.html.
step 2500: train loss 0.9585, val loss 1.5002
iter 2500: loss 1.0777, time 18685.18ms, mfu 7.08%
iter 2510: loss 1.0394, time 50.00ms, mfu 7.12%
iter 2520: loss 1.0681, time 48.98ms, mfu 7.17%
iter 2530: loss 1.0279, time 50.10ms, mfu 7.19%
iter 2540: loss 1.0509, time 49.40ms, mfu 7.23%
iter 2550: loss 1.0752, time 48.13ms, mfu 7.28%
iter 2560: loss 1.0457, time 50.71ms, mfu 7.29%
iter 2570: loss 1.0512, time 50.16ms, mfu 7.30%
iter 2580: loss 1.0435, time 50.40ms, mfu 7.31%
iter 2590: loss 1.0404, time 50.27ms, mfu 7.32%
iter 2600: loss 1.0099, time 49.79ms, mfu 7.34%
iter 2610: loss 1.0434, time 50.14ms, mfu 7.35%
iter 2620: loss 1.0196, time 49.92ms, mfu 7.36%
iter 2630: loss 1.0512, time 50.45ms, mfu 7.36%
iter 2640: loss 1.0673, time 49.01ms, mfu 7.39%
iter 2650: loss 1.0164, time 50.31ms, mfu 7.39%
iter 2660: loss 1.0378, time 50.37ms, mfu 7.39%
iter 2670: loss 1.0402, time 50.23ms, mfu 7.39%
iter 2680: loss 1.0082, time 49.82ms, mfu 7.40%
iter 2690: loss 1.0065, time 50.37ms, mfu 7.40%
iter 2700: loss 1.0545, time 49.66ms, mfu 7.41%
iter 2710: loss 1.0216, time 50.25ms, mfu 7.41%
iter 2720: loss 1.0098, time 50.98ms, mfu 7.40%
iter 2730: loss 1.0095, time 49.62ms, mfu 7.41%
iter 2740: loss 1.0097, time 49.01ms, mfu 7.43%
step 2750: train loss 0.9092, val loss 1.5216
iter 2750: loss 1.0217, time 18552.53ms, mfu 6.69%
iter 2760: loss 1.0108, time 49.78ms, mfu 6.77%
iter 2770: loss 0.9920, time 50.86ms, mfu 6.82%
iter 2780: loss 1.0201, time 49.53ms, mfu 6.89%
iter 2790: loss 1.0046, time 50.12ms, mfu 6.95%
iter 2800: loss 1.0014, time 50.02ms, mfu 7.00%
iter 2810: loss 0.9883, time 51.27ms, mfu 7.03%
iter 2820: loss 0.9922, time 48.38ms, mfu 7.09%
iter 2830: loss 1.0057, time 49.84ms, mfu 7.13%
iter 2840: loss 0.9884, time 49.98ms, mfu 7.16%
iter 2850: loss 0.9904, time 49.70ms, mfu 7.20%
iter 2860: loss 1.0043, time 50.24ms, mfu 7.22%
iter 2870: loss 0.9590, time 50.27ms, mfu 7.24%
iter 2880: loss 0.9890, time 51.03ms, mfu 7.25%
iter 2890: loss 0.9845, time 49.05ms, mfu 7.28%
iter 2900: loss 0.9998, time 49.08ms, mfu 7.31%
iter 2910: loss 0.9801, time 50.85ms, mfu 7.31%
iter 2920: loss 0.9818, time 51.07ms, mfu 7.31%
iter 2930: loss 0.9819, time 49.20ms, mfu 7.34%
iter 2940: loss 0.9739, time 50.67ms, mfu 7.34%
iter 2950: loss 0.9449, time 48.87ms, mfu 7.37%
iter 2960: loss 0.9820, time 50.84ms, mfu 7.36%
Traceback (most recent call last):
  File "train.py", line 305, in <module>
    scaler.scale(loss).backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 503, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt